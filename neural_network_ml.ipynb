{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow and numpy to create the neural network\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib to plot info to show our results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OS to load files and save checkpoints\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST data from tf examples\n",
    "\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "\n",
    "color_channels = 1\n",
    "\n",
    "model_name = \"mnist\"\n",
    "\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "\n",
    "train_data = mnist.train.images\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "\n",
    "# Load all the data batches\n",
    "eval_data = mnist.test.images\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "category_names = list(map(str, range(10)))\n",
    "\n",
    "# TODO: Process mnist data\n",
    "print(train_data.shape)\n",
    "\n",
    "train_data = np.reshape(train_data, (-1, image_height, image_width, color_channels))\n",
    "\n",
    "print(train_data.shape)\n",
    "\n",
    "eval_data = np.reshape(eval_data, (-1, image_height, image_width, color_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cifar data from file\n",
    "\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "\n",
    "color_channels = 3\n",
    "\n",
    "model_name = \"cifar\"\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "cifar_path = '../cifar-10-data/'\n",
    "\n",
    "\n",
    "\n",
    "# Load the english category names.\n",
    "category_names_bytes = unpickle(cifar_path + 'batches.meta')[b'label_names']\n",
    "category_names = list(map(lambda x: x.decode(\"utf-8\"), category_names_bytes))\n",
    "\n",
    "# TODO: Process Cifar data\n",
    "\n",
    "def process_data(data):\n",
    "    float_data = np.array(data, dtype=np.float32) / 255.0\n",
    "    \n",
    "    reshaped_data = np.reshape(float_data, (-1, color_channels, image_height, image_width))\n",
    "    \n",
    "    # The incorrect image\n",
    "    \n",
    "    transposed_data = np.transpose(reshaped_data, [0, 2, 3, 1])\n",
    "    \n",
    "    return transposed_data\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "train_filename = \"../cifar-10-data/train.tfrecords\"\n",
    "\n",
    "if not os.path.isfile(train_filename):\n",
    "    # If training data isn't rfrecorded yet, do so.\n",
    "    \n",
    "    train_data = np.array([])\n",
    "    train_labels = np.array([], dtype=np.int64)\n",
    "    \n",
    "    # Load all the data batches.\n",
    "    for i in range(1,6):\n",
    "        data_batch = unpickle(cifar_path + 'data_batch_' + str(i))\n",
    "        train_data = np.append(train_data, data_batch[b'data'])\n",
    "        train_labels = np.append(train_labels, data_batch[b'labels'])\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "    \n",
    "    train_data = process_data(train_data)\n",
    "    \n",
    "    for index in range(len(train_data)):\n",
    "        feature = {'image': _bytes_feature(tf.compat.as_bytes(train_data[index].tostring())),\n",
    "                   'label': _int64_feature(train_labels[index])}\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    print(\"Training data generated in \" + train_filename)\n",
    "\n",
    "\n",
    "# Load the eval batch.\n",
    "eval_batch = unpickle(cifar_path + 'test_batch')\n",
    "\n",
    "eval_data = eval_batch[b'data']\n",
    "eval_labels = eval_batch[b'labels'] \n",
    "\n",
    "eval_data = process_data(eval_data)\n",
    "\n",
    "# Function to parse a training tfrecord\n",
    "def parse_cifar_record(record):\n",
    "    features = tf.parse_single_example(\n",
    "        record,\n",
    "        features={\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "        })\n",
    "    image = tf.decode_raw(features['image'], tf.float32)\n",
    "    image.set_shape([color_channels * image_height * image_width])\n",
    "    image = tf.reshape(image, [image_height, image_width, color_channels])\n",
    "    label = tf.cast(features['label'], tf.float32)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The neural network\n",
    "class ConvNet:\n",
    "    \n",
    "    def __init__(self, image_height, image_width, channels, num_classes):\n",
    "        \n",
    "        self.input_layer = tf.placeholder(dtype=tf.float32, shape=[None, image_height, image_width, channels], name=\"inputs\")\n",
    "        print(self.input_layer.shape)  # it's important to print each tensor for debugging\n",
    "        \n",
    "        conv_layer_1 = tf.layers.conv2d(self.input_layer, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "        print(conv_layer_1.shape)\n",
    "        \n",
    "        pooling_layer_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=[2,2], strides=2)\n",
    "        print(pooling_layer_1.shape)\n",
    "        \n",
    "        conv_layer_2 = tf.layers.conv2d(pooling_layer_1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "        print(conv_layer_2.shape)\n",
    "        \n",
    "        pooling_layer_2 = tf.layers.max_pooling2d(conv_layer_2, pool_size=[2, 2], strides=2)\n",
    "        print(pooling_layer_2.shape)\n",
    "        \n",
    "        flattened_pooling = tf.layers.flatten(pooling_layer_2)\n",
    "        dense_layer = tf.layers.dense(flattened_pooling, 1024, activation=tf.nn.relu)\n",
    "        print(dense_layer.shape)\n",
    "        dropout = tf.layers.dropout(dense_layer, rate=0.4, training=True)\n",
    "        outputs = tf.layers.dense(dropout, num_classes)\n",
    "        print(outputs.shape)\n",
    "        \n",
    "        self.choice = tf.argmax(outputs, axis=1)\n",
    "        self.probability = tf.nn.softmax(outputs)\n",
    "        \n",
    "        self.labels = tf.placeholder(dtype=tf.float32, name=\"labels\")\n",
    "        self.accuracy, self.accuracy_op = tf.metrics.accuracy(self.labels, self.choice)\n",
    "        \"\"\"self.accuracy stores the accuracy, _op stores the models accuracy\"\"\"\n",
    "        \n",
    "        one_hot_labels = tf.one_hot(indices=tf.cast(self.labels, dtype=tf.int32), depth=num_classes)     \n",
    "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits=outputs)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2)\n",
    "        self.train_operation = optimizer.minimize(loss=self.loss, global_step=tf.train.get_global_step())\n",
    "        \"\"\"\n",
    "        argmax: finds the index of the highest weight element in a tensor, returns choice of class\n",
    "        outputs: sizes final layer based on the possible classifications to compute the outputs, returns output weights\n",
    "        softmax: returns a tensor with decimal probabilities of each element\n",
    "\n",
    "        softmax and cross_entropy are used as a loss function to train network\n",
    "\n",
    "        convolutional networks are built of a few basic layers:\n",
    "            > convolutional layers to learn features\n",
    "            > pooling layers to simplify data\n",
    "            > dense layers at the end to output a decision\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: initialize variables\n",
    "training_steps = 10000  # how long training loop will run (20000)\n",
    "batch_size = 32  # how many images will be passed into the network at each step of training (64)\n",
    "\n",
    "path = \"./\" + model_name + \"-cnn/\"  # path to save neural networks training\n",
    "\n",
    "load_checkpoint = False  # toggle to set whether load a past training model\n",
    "performance_graph = np.array([])  # for graph's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the training loop\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)) # created from existing tensor or list\n",
    "# dataset = dataset.shuffle(buffer_size=train_labels.shape[0])  # shuffles dataset\n",
    "# dataset = dataset.batch(batch_size) # batches dataset into batches\n",
    "# dataset = dataset.repeat()  # sets the dataset to loop repeatedly\n",
    "if model_name == \"mnist\":\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))    \n",
    "    dataset = dataset.shuffle(buffer_size=train_labels.shape[0])\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat()\n",
    "else:\n",
    "    filenames = [\"../cifar-10-data/train.tfrecords\"]\n",
    "    dataset = tf.data.TFRecordDataset(filenames).repeat()\n",
    "    dataset = dataset.map(parse_cifar_record)\n",
    "    dataset = dataset.shuffle(buffer_size=45000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "# dataset_iterator = dataset.make_one_shot_iterator()  # creates an iterator\n",
    "dataset_iterator = dataset.make_initializable_iterator()\n",
    "next_element = dataset_iterator.get_next()  # saves its get_next operation\n",
    "\n",
    "# TODO: Setting Up the Neural Network\n",
    "cnn = ConvNet(image_height,image_width,color_channels,10)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "\"\"\" The saver class allows weights and vars in TF graphs to be saved between runs.\n",
    "    The 'max_to_keep' controls the number of checkpoints to save, low number saves disk space\n",
    "\"\"\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Load and initialize vars into TF session\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # either load from checkpoint or init weights\n",
    "    if load_checkpoint:\n",
    "        checkpoint = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "    else:  # if on a new network, runs global initialize\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # initialize local vars for the accuracy metrics calculation\n",
    "    sess.run(tf.local_variables_initializer())  # run local initializer\n",
    "    sess.run(dataset_iterator.initializer)  # initialize the iterator\n",
    "    \n",
    "    # loop through and train\n",
    "    for step in range(training_steps):\n",
    "        current_batch = sess.run(next_element)  # get the current batch\n",
    "        \n",
    "        batch_inputs = current_batch[0]  # split the resulting batch into the image inputs and...\n",
    "        batch_labels = current_batch[1]     # the image labels\n",
    "        \n",
    "        # run networks training operation and the accuracy operation\n",
    "        sess.run((cnn.train_operation, cnn.accuracy_op), feed_dict={cnn.input_layer:batch_inputs, cnn.labels:batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            performance_graph = np.append(performance_graph, sess.run(cnn.accuracy))  # updates list every 10 steps\n",
    "        \n",
    "        if step % 1000 == 0 and step > 0:\n",
    "            current_acc = sess.run(cnn.accuracy)  # gets current accuracy\n",
    "            print(\"Accuracy at step \" + str(step) + \": \" + str(current_acc))\n",
    "            print(\"Saving checkpoint\")\n",
    "            saver.save(sess, path + model_name, step)  # save a checkpoint\n",
    "        \n",
    "    print(\"Saving final checkpoint for training session.\")\n",
    "    saver.save(sess, path + model_name, step)\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display graph of performance over time\n",
    "plt.figure().set_facecolor('white')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(performance_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand this code box to check your work!# TODO: Run through the evaluation data set, check accuracy of model\n",
    "with tf.Session() as sess:\n",
    "    checkpoint = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess,checkpoint.model_checkpoint_path)  # load saved checkpoint\n",
    "    \n",
    "    sess.run(tf.local_variables_initializer())  # initialize local vars\n",
    "    \n",
    "    for image, label in zip(eval_data, eval_labels):\n",
    "        sess.run(cnn.accuracy_op, feed_dict={cnn.input_layer:[image], cnn.labels:label})\n",
    "    \n",
    "    print(sess.run(cnn.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from indices, display the network's guess and label.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    checkpoint = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess,checkpoint.model_checkpoint_path)\n",
    "    \n",
    "    indexes = np.random.choice(len(eval_data), 10, replace=False)\n",
    "    \n",
    "    rows = 5\n",
    "    cols = 2\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5,5))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    image_count = 0\n",
    "    \n",
    "    for idx in indexes:\n",
    "        image_count += 1\n",
    "        sub = plt.subplot(rows,cols,image_count)\n",
    "        img = eval_data[idx]\n",
    "        if model_name == \"mnist\":\n",
    "            img = img.reshape(28, 28)\n",
    "        plt.imshow(img)\n",
    "        guess = sess.run(cnn.choice, feed_dict={cnn.input_layer:[eval_data[idx]]})\n",
    "        if model_name == \"mnist\":\n",
    "            guess_name = str(guess[0])\n",
    "            actual_name = str(eval_labels[idx])\n",
    "        else:\n",
    "            guess_name = category_names[guess[0]]\n",
    "            actual_name = category_names[eval_labels[idx]]\n",
    "        sub.set_title(\"G: \" + guess_name + \" A: \" + actual_name)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Session().close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
